{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 715,
     "status": "ok",
     "timestamp": 1601363899867,
     "user": {
      "displayName": "kustar2020",
      "photoUrl": "",
      "userId": "15258523740669012590"
     },
     "user_tz": -540
    },
    "id": "jlN2b0A_t7IT"
   },
   "outputs": [],
   "source": [
    "data_dir = #'data 저장된 곳 주소'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 676,
     "status": "ok",
     "timestamp": 1601363901268,
     "user": {
      "displayName": "kustar2020",
      "photoUrl": "",
      "userId": "15258523740669012590"
     },
     "user_tz": -540
    },
    "id": "QKNNw_npt7IW"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C01YNPUt7IZ"
   },
   "source": [
    "<h2> Sentiment_train.txt / Sentiment_test.txt 파일을 읽고 문장과 정답을 분리하여 각 리스트에 저장</h2>\n",
    "<pre>\n",
    "<b>1. 데이터 예시</b>\n",
    "    문장 \\t(tab) 라벨 ([P] : 긍정, [N] : 부정)\n",
    "\n",
    "    예시)\n",
    "      제품 도 너무 너무 맘 에 들 ㄴ답니다\\t[P]\n",
    "      집사 라 미 드럼 세탁기 를 싫 어 하 ㅂ니다\\t[N]\n",
    "      너무 좋 아요\\t[P]\n",
    "      조금 작 은 감 이 있 지만 잘 쓰 ㄹ께 요\\t[N]\n",
    "<b>2. 반환 형태</b>\n",
    "    texts = [문장1, 문장2, 문장3, ... ]\n",
    "    labels = [문장1에 대한 라벨, 문장2에 대한 라벨, ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 643,
     "status": "ok",
     "timestamp": 1601363903667,
     "user": {
      "displayName": "kustar2020",
      "photoUrl": "",
      "userId": "15258523740669012590"
     },
     "user_tz": -540
    },
    "id": "zhq4-VHBt7IZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_data(file_path):\n",
    "  file = open(os.path.join(data_dir, file_path), encoding='UTF-8')\n",
    "\n",
    "  texts, labels = [], []\n",
    "\n",
    "  for line in tqdm(file.readlines()):\n",
    "    text, label = line.strip().split('\\t')\n",
    "    texts.append(text)\n",
    "    labels.append(label)\n",
    "  return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 1726,
     "status": "ok",
     "timestamp": 1601363907174,
     "user": {
      "displayName": "kustar2020",
      "photoUrl": "",
      "userId": "15258523740669012590"
     },
     "user_tz": -540
    },
    "id": "Fav1f_txt7Ic",
    "outputId": "a6df116a-197a-4364-fbd2-44e676d82cc1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2686/2686 [00:00<00:00, 495247.96it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 505134.97it/s]\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y = load_data(\"sentiment_train.txt\")\n",
    "test_X, test_y = load_data(\"sentiment_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3TMjrMNt7Ie"
   },
   "source": [
    "<h2> 문장을 벡터로 표현하기</h2>\n",
    "<pre>\n",
    "<b>1. CountVectorizer()</b>\n",
    "    문서에 나타나는 단어 빈도수를 기반으로 Vocabulary 생성 및 벡터 변환하기 위한 객체\n",
    "\n",
    "    사용 함수)\n",
    "      ~.fit_transform(X)\n",
    "        문서 리스트 X에 등장하는 단어를 기반으로 Vocabulary 자동 생성 및 X를 생성된 Vocabulary에 매핑하여 return\n",
    "      ~.transform(X)\n",
    "        CountVectorizer 객체 내에 존재하는 Vocabulary를 기반으로 문장 리스트 X를 벡터로 변환\n",
    "<b>2. 반환 형태</b>\n",
    "    train_X = ['나 는 밥 을 먹는다', '나 는 학생 이야', '학생 은 학교 에 다녀']\n",
    "    feature_train_X = CountVectorizer.fit_transform(train_X)\n",
    "    : [[1, 1, 0], [1, 1, 1], [0, 0, 1]\n",
    "    Vocabulary\n",
    "    : [나:0, 는:1, 학생:2]\n",
    "    test_X = ['나 학생 학생, 나, 나']\n",
    "    feature_test_X = CountVectorizer.transform(test_X)\n",
    "    : [3, 0, 2]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 650,
     "status": "ok",
     "timestamp": 1601363910996,
     "user": {
      "displayName": "kustar2020",
      "photoUrl": "",
      "userId": "15258523740669012590"
     },
     "user_tz": -540
    },
    "id": "cQi7tFCht7If"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "feature_train_X = vectorizer.fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 611,
     "status": "ok",
     "timestamp": 1601363912317,
     "user": {
      "displayName": "kustar2020",
      "photoUrl": "",
      "userId": "15258523740669012590"
     },
     "user_tz": -540
    },
    "id": "inNgBfLat7Ih",
    "outputId": "966006cc-46b7-4835-fe57-6ca254f60f9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(feature_train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 692,
     "status": "ok",
     "timestamp": 1601363913676,
     "user": {
      "displayName": "kustar2020",
      "photoUrl": "",
      "userId": "15258523740669012590"
     },
     "user_tz": -540
    },
    "id": "t2MyDgSKt7Ij"
   },
   "outputs": [],
   "source": [
    "feature_test_X = vectorizer.transform(test_X)\n",
    "predictions = classifier.predict(feature_test_X).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 636,
     "status": "ok",
     "timestamp": 1601363917260,
     "user": {
      "displayName": "kustar2020",
      "photoUrl": "",
      "userId": "15258523740669012590"
     },
     "user_tz": -540
    },
    "id": "RngEV4BPt7Im",
    "outputId": "d539bd8b-1190-4845-8e1d-3589e4bfd014"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy: %.2f' % accuracy_score(test_y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwnMajPjuof3"
   },
   "outputs": [],
   "source": [
    "test_sent = vectorizer.inverse_transform(feature_test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHJlK4pot7Io"
   },
   "source": [
    "<h2>\"SMSSpamCollection\" 데이터를 읽고 문장과 정답을 분리하여 각 리스트에 저장</h2>\n",
    "\n",
    "<pre>\n",
    "<b>1. 데이터의 형태(SMSSpamCollection)</b>\n",
    "  라벨(스팸 또는 햄) \\t(tab) 문장 \n",
    "  \n",
    "  위와 같은 형태로 저장되어 있음\n",
    "  \n",
    "  예시)\n",
    "    ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
    "    spam\\tCustomer service annoncement. You have a New Years delivery waiting for you. Please call 07046744435 now to arrange delivery\n",
    "    ...\n",
    "  \n",
    "  따라서 입력 데이터를 읽고 \\t을 기준으로 입력 문장을 분리한 후에 문장과 라벨을 각각 x_data, y_data 리스트에 저장\n",
    "\n",
    "<b>2. x_data, y_data 형태</b>\n",
    "  x_data = [ 문장1, 문장2, 문장3, ... ]\n",
    "  y_data = [ 문장1의 라벨, 문장2의 라벨, 문장3의 라벨, ... ]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 641,
     "status": "ok",
     "timestamp": 1601364051005,
     "user": {
      "displayName": "kustar2020",
      "photoUrl": "",
      "userId": "15258523740669012590"
     },
     "user_tz": -540
    },
    "id": "v-q4Pws4t7Io",
    "outputId": "3451ea45-784d-4415-d547-e49bf0a8cecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data의 개수 : 1500\n",
      "y_data의 개수 : 1500\n"
     ]
    }
   ],
   "source": [
    "file_path = # \"data 저장된 곳 주소 /SMSSpamCollection\"\n",
    "\n",
    "# 파일 읽기\n",
    "with open(file_path,'r',encoding='utf8') as inFile:\n",
    "  lines = inFile.readlines()\n",
    "\n",
    "x_data, y_data = [], []\n",
    "for line in lines:\n",
    "  pieces = line.strip().split('\\t')\n",
    "  label, sentence = pieces[0], pieces[1]\n",
    "  \n",
    "  x_data.append(sentence)\n",
    "  y_data.append(label)\n",
    "  \n",
    "print(\"x_data의 개수 : \" + str(len(x_data)))\n",
    "print(\"y_data의 개수 : \" + str(len(y_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-bnWDlZt7Ir"
   },
   "source": [
    "<h2>Tokenizer 라이브러리를 사용하여 입력 문장을 index로 치환</h2>\n",
    "\n",
    "<pre>\n",
    "<b>1. tokenizer.fit_on_texts(data) 함수를 이용하여 각 단어를 index로 치환하기 위한 딕셔너리 생성</b>\n",
    "   생성된 딕셔너리는 tokenizer 객체 안에 저장됨\n",
    "  \n",
    "  tokenizer.fit_on_texts(data)\n",
    "  args\n",
    "    data : 문자열 element를 가지고 있는 리스트\n",
    "  return\n",
    "    X\n",
    "    \n",
    "  딕셔너리 예시)\n",
    "    {'to': 1, 'i': 2, 'you': 3, 'a': 4, 'the': 5, 'and': 6, 'for': 7 ... }\n",
    "    \n",
    "<b>2. tokenizer.texts_to_matrix(data) 함수를 이용하여 각 단어를 one-hot 벡터로 표현하며 문장은 각 단어의 one-hot 벡터의 합으로 표현</b>\n",
    "\n",
    "  tokenizer.texts_to_sequences(data)\n",
    "  args\n",
    "    data : 문자열 element를 가지고 있는 리스트\n",
    "  return : \n",
    "    indexing 된 리스트\n",
    "\n",
    "  예시)\n",
    "    전체 단어 사전 : {'i': 1, 'am': 2, 'happy': 3, 'sad': 4}\n",
    "    i     : [1, 0, 0, 0]\n",
    "    am    : [0, 1, 0, 0]\n",
    "    happy : [0, 0, 1, 0]\n",
    "    sad   : [0, 0, 0, 1]\n",
    "    \n",
    "    문장 1 : i am happy   -> [1, 1, 1, 0]\n",
    "    문장 2 : i am sad     -> [1, 1, 0, 1]\n",
    "    문장 3 : i am sad sad -> [1, 1, 0, 2]\n",
    "    \n",
    "  indexing 예시)\n",
    "    x_data indexing 하기 전 : Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
    "    x_data indexing 하기 후 : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    y_data indexing 하기 전 : ham\n",
    "    y_data indexing 하기 후 : 1\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "executionInfo": {
     "elapsed": 2562,
     "status": "ok",
     "timestamp": 1601364062463,
     "user": {
      "displayName": "kustar2020",
      "photoUrl": "",
      "userId": "15258523740669012590"
     },
     "user_tz": -540
    },
    "id": "hIYz6lJ_t7It",
    "outputId": "a3f89e70-afb4-40fc-a1ae-ebeecc8777ad",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data indexing 전 : Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "x_data indexing 후 : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "y_data indexing 전 : ham\n",
      "y_data indexing 후 : 1\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "# spam, ham 라벨을 대응하는 index로 치환하기위한 딕셔너리\n",
    "label2index = {'spam':0, 'ham':1}\n",
    "index2label = {0:\"spam\", 1:\"ham\"}\n",
    "\n",
    "# indexing 한 데이터를 넣을 리스트 선언\n",
    "indexing_x_data, indexing_y_data = [], []\n",
    "\n",
    "for label in y_data:\n",
    "  indexing_y_data.append(label2index[label])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=300)\n",
    "\n",
    "# x_data를 사용하여 딕셔너리 생성\n",
    "tokenizer.fit_on_texts(x_data)                \n",
    "\n",
    "# x_data에 있는 각 문장들을 one-hot 벡터의 합으로 치환하고 그 결과값을 indexing_x_data에 저장\n",
    "indexing_x_data = tokenizer.texts_to_matrix(x_data, mode='count').tolist()\n",
    "\n",
    "print(\"x_data indexing 전 : \" + str(x_data[0]))\n",
    "print(\"x_data indexing 후 : \" + str(indexing_x_data[0]))\n",
    "print(\"y_data indexing 전 : \" + str(y_data[0]))\n",
    "print(\"y_data indexing 후 : \" + str(indexing_y_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gerUG08t7Iw"
   },
   "source": [
    "<h2>SVM 모델 학습</h2>\n",
    "\n",
    "<pre>    \n",
    "<b>1. 입력 데이터를 9 대 1 비율로 나누어 학습, 평가에 사용</b>\n",
    "  \n",
    "<b>2. svm.fit(x, y) 함수를 사용하여 SVM 모델 학습</b>\n",
    "  svm.fit(x, y)\n",
    "  args\n",
    "    x : indexing 된 문장들이 있는 리스트\n",
    "    y : x의 각 문장에 대응하는 라벨이 있는 리스트\n",
    "  return : \n",
    "    X\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "executionInfo": {
     "elapsed": 951,
     "status": "ok",
     "timestamp": 1601364066279,
     "user": {
      "displayName": "kustar2020",
      "photoUrl": "",
      "userId": "15258523740669012590"
     },
     "user_tz": -540
    },
    "id": "P9wtcDqtt7Iw",
    "outputId": "55413b5e-3a98-4403-d29e-5d9bd8cc7dd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x의 개수 : 1350\n",
      "train_y의 개수 : 1350\n",
      "test_x의 개수 : 150\n",
      "test_y의 개수 : 150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# 전체 데이터를 9:1의 비율로 나누어 학습 및 평가 데이터로 사용\n",
    "number_of_train_data = int(len(indexing_x_data)*0.9)\n",
    "\n",
    "train_x, train_y = indexing_x_data[:number_of_train_data], indexing_y_data[:number_of_train_data]\n",
    "test_x, test_y = indexing_x_data[number_of_train_data:], indexing_y_data[number_of_train_data:]\n",
    "\n",
    "print(\"train_x의 개수 : \" + str(len(train_x)))\n",
    "print(\"train_y의 개수 : \" + str(len(train_y)))\n",
    "print(\"test_x의 개수 : \" + str(len(test_x)))\n",
    "print(\"test_y의 개수 : \" + str(len(test_y)))\n",
    "\n",
    "svm = SVC(kernel='rbf')\n",
    "svm.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPigLnXtvPuY"
   },
   "source": [
    "<h2>SVM 모델을 이용한 평가</h2>\n",
    "\n",
    "<pre>\n",
    "<b>1. svm.predict(data) 함수를 사용하여 SVM 모델을 이용하여 평가</b>\n",
    "  \n",
    "  svm.predict(data)\n",
    "  args\n",
    "    data : indexing 된 문장들이 있는 리스트\n",
    "  return : \n",
    "    입력 문장들에 대한 모델의 출력 라벨 리스트\n",
    "    \n",
    "<b>2. 성능 측정</b>\n",
    "  정답 라벨과 모델의 출력 라벨을 비교하여 성능 측정\n",
    "  \n",
    "<b>3. tokenizer.sequences_to_texts(data) 함수를 이용하여 indexing 된 데이터를 단어로 치환</b>\n",
    "\n",
    "  tokenizer.sequences_to_texts(data)\n",
    "  args\n",
    "    data : indexing 된 리스트\n",
    "  return : \n",
    "    단어로 치환된 리스트\n",
    "    \n",
    "  예시)\n",
    "    [38, 93, 239, 240, 241, 242, 53, 11, 243, 72, 94, 244, 245, 126, 246, 247, 73, 74, 248, 127] -> Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
    "    \n",
    "<b>4. 입력 문장에 대한 모델의 출력과 정답 출력</b>\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "executionInfo": {
     "elapsed": 657,
     "status": "ok",
     "timestamp": 1601364128678,
     "user": {
      "displayName": "kustar2020",
      "photoUrl": "",
      "userId": "15258523740669012590"
     },
     "user_tz": -540
    },
    "id": "3byDFwUyvLqv",
    "outputId": "d62f4bb9-de63-4ee0-e787-a9616524115b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n",
      "\n",
      "문장 :  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "정답 :  spam\n",
      "모델 출력 :  spam\n",
      "\n",
      "문장 :  Ok lar... Joking wif u oni...\n",
      "정답 :  ham\n",
      "모델 출력 :  ham\n",
      "\n",
      "문장 :  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "정답 :  ham\n",
      "모델 출력 :  ham\n",
      "\n",
      "문장 :  U dun say so early hor... U c already then say...\n",
      "정답 :  ham\n",
      "모델 출력 :  ham\n",
      "\n",
      "문장 :  Nah I don't think he goes to usf, he lives around here though\n",
      "정답 :  ham\n",
      "모델 출력 :  ham\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "predict = svm.predict(test_x)\n",
    "\n",
    "print('Accuracy: %.2f' % accuracy_score(test_y, predict))\n",
    "for index in range(len(x_data[number_of_train_data:number_of_train_data+5])):\n",
    "  print()\n",
    "  print(\"문장 : \", x_data[index])\n",
    "  print(\"정답 : \", index2label[test_y[index]])\n",
    "  print(\"모델 출력 : \", index2label[predict[index]])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "2020_02_Kuggle_정규세션_W4_0929.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
